#!/usr/bin/env python3

import requests
import re
import sys
import argparse
import time

# Lists for findings of each type
boulders = []
sports = []
trads = []
topropes = []

# Define sorting orders for boulders and routes
boulder_order = ['V-easy','VB','V-','V0','V0+','V0-1','V1-','V1','V1+','V1-2','V2-','V2','V2+','V2-3','V3-','V3','V3+','V3-4','V4-','V4','V4+','V4-5','V5-','V5','V5+','V5-6','V6-','V6','V6+','V6-7','V7-','V7','V7+','V7-8','V8-','V8','V8+','V8-9','V9-','V9','V9+','V9-10','V10-','V10','V10+','V10-11','V11-','V11','V11+','V11-12','V12-','V12','V12+','V12-13','V13-','V13','V13+','V13-14','V14-','V14','V14+','V14-15','V15-','V15','V15+','V15-16','V16-','V16','V16-17','V17-','V17']

route_order = ['5.4','5.5-','5.5','5.5+','5.6-','5.6','5.6+','5.7-','5.7','5.7+','5.8-','5.8','5.8+','5.9-','5.9','5.9+','5.10a','5.10-','5.10b','5.10','5.10c','5.10+','5.10d','5.11a','5.11-','5.11b','5.11','5.11c','5.11+','5.11d','5.12a','5.12-','5.12b','5.12','5.12c','5.12+','5.12d','5.13a','5.13-','5.13b','5.13','5.13c','5.13+','5.13d','5.14a','5.14-','5.14b','5.14','5.14c','5.14+','5.14d','5.15a','5.15-','5.15b','5.15','5.15c','5.15+','5.15d']

# Set of seen links
visited = set()

# Types of link we don't want to follow...
ignore = ['google','facebook','login','user','help','edit','share','upload','add','photo','forum','stats','print','map']

# Get links from a html page
def get_links(content):
  links = re.findall('''<a\s+(?:[^>]*?\s+)?href="([^"]*)"''', content)  
  return links

# Get boulders from an HTML page
def get_climbs(content):
  info = re.findall('''@type.*\s*\"name\": \"([^\"]*).*\s*\"description\": \"([^\"]*)''',content)
  rating = re.findall('''&nbsp;Avg: (...)''',content)
  if len(rating) > 0 and len(info) > 0:
    rating = rating[0]
    if rating[-1] == 'f':
      rating = rating[0]
    name = info[0][0]
    type = info[0][1]
    type = type.split(" ")
    grade = type[0]
    type = type[1:]
    #print("Type: " + str(type))
    if 'Sport' in type:
      sports.append((grade,rating))
    if 'Boulder' in type:
      boulders.append((grade,rating))
    if 'TR' in type:
      topropes.append((grade,rating))
    if 'Trad' in type:
      trads.append((grade,rating))
    print(name + " " + str(type) + " " + grade + " " + rating + "/4 stars")


# Crawl a link
def crawl(url):
  if len(boulders) + len(sports) + len(trads) + len(topropes) < num_climbs:
    response = s.get(url)
    if response == None:
      return

    page_content = response.content.decode('latin-1')
   
    # Get all links on the page
    links = get_links(page_content)

    # Add the climbs on the page
    get_climbs(page_content)

    # Remove links which we dont want to follow
    for word in ignore:
      links = [link for link in links if not word in link]

    # Only keep route and are links, this will get us all climbs
    links = [link for link in links if 'mountainproject' in link and ('area' in link or 'route' in link)]

    # Add the new links to our set so we don't visit them again
    for link in links:
      if not link in visited:
        visited.add(link)
        #print an update every [num] links
        if len(visited) % 500 == 0:
          print("\n***\n" + str(len(visited)) + " pages searched... " + str(len(boulders)) + " boulders found... " + str(len(sports)) + " sport routes found... " + str(len(trads)) + " trad routes found... " + str(len(topropes)) + " toprope routes found... \n***\n")
        #write results to a file every [num] links (in case it gets blocked before completing)
        if len(visited) % 200 == 0:
          write_results()
        crawl(link)

# Prints information about a specific type of climb
def get_type_info(climbs,order,type):
  result = ""
  for grade in order:
    sum = 0
    count = 0
    for climb in climbs:
      if climb[0] == grade:
        count += 1
        sum += float(climb[1])
    if count > 0:
      avg = float(float(sum) / float(count))
      result += "Average stars for (" + str(count) + ") " + grade + " " + type + ": " + str(round(avg,2)) + "\n"
  return result


# Write meta info to a file
def write_results():
  f = open("output.txt","w")
  f.write('\n\n\nFound in total: ' + str(len(boulders)) + ' boulders, ' + str(len(sports)) + ' sport climbs, ' + str(len(trads)) + ' trad climbs, and ' + str(len(topropes)) + ' toprope climbs.')
  f.write('\n\n*******************\n* RATING AVERAGES *\n*******************')
  f.write('\n\n\nBOULDER AVERAGES\n')
  f.write(get_type_info(boulders,boulder_order,'boulders'))
  f.write('\n\n\nSPORT CLIMB AVERAGES\n')
  f.write(get_type_info(sports,route_order,'sport climbs'))
  f.write('\n\n\nTRAD CLIMB AVERAGES\n')
  f.write(get_type_info(trads,route_order,'trad climbs'))
  f.write('\n\n\nTOPROPE CLIMB AVERAGES\n')
  f.write(get_type_info(topropes,route_order,'toprope climbs'))
  f.close()

if __name__ == '__main__':
  sys.setrecursionlimit(100000)

  parser = argparse.ArgumentParser()
  parser.add_argument("num_climbs",help="Specifiy the number of climbs to index here")
  args = parser.parse_args()

  num_climbs = int(args.num_climbs)

  # Set up a session
  s = requests.session()

  crawl('https://www.mountainproject.com')

  write_results()
