#!/usr/bin/env python3

import requests
import re
import sys
import argparse
import time
from itertools import cycle

# Lists for findings of each type
boulders = []
sports = []
trads = []
topropes = []

# Define sorting orders for boulders and routes
boulder_order = ['V-easy','VB','V-','V0','V0+','V0-1','V1-','V1','V1+','V1-2','V2-','V2','V2+','V2-3','V3-','V3','V3+','V3-4','V4-','V4','V4+','V4-5','V5-','V5','V5+','V5-6','V6-','V6','V6+','V6-7','V7-','V7','V7+','V7-8','V8-','V8','V8+','V8-9','V9-','V9','V9+','V9-10','V10-','V10','V10+','V10-11','V11-','V11','V11+','V11-12','V12-','V12','V12+','V12-13','V13-','V13','V13+','V13-14','V14-','V14']

route_order = ['5.4','5.5-','5.5','5.5+','5.6-','5.6','5.6+','5.7-','5.7','5.7+','5.8-','5.8','5.8+','5.9-','5.9','5.9+','5.10-','5.10a','5.10-','5.10b','5.10','5.10c','5.10+','5.10d','5.11a','5.11-','5.11b','5.11','5.11c','5.11+','5.11d','5.12a','5.12-','5.12b','5.12','5.12c','5.12+','5.12d','5.13a','5.13-','5.13b','5.13','5.13c','5.13+','5.13d','5.14a','5.14-','5.14b','5.14','5.14c','5.14+','5.14d']

# Set of seen links
visited = set()

# Types of link we don't want to follow...
ignore = ['google','facebook','login','user','help','edit','share','upload','add','photo','forum','stats','print','map']

# Get proxies to route traffic through
def get_proxies(session):
  url = 'https://free-proxy-list.net/'
  response = s.get(url)
  content = response.content.decode('latin-1')
  proxies = re.findall('''(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}).{9}(\d{1,5})''', content)
  proxies = [proxy[0] + ':' + proxy[1] for proxy in proxies]
  return proxies

# Get links from a html page
def get_links(content):
  links = re.findall('''<a\s+(?:[^>]*?\s+)?href="([^"]*)"''', content)  
  return links

# Get boulders from an HTML page
def get_climbs(content):
  info = re.findall('''@type.*\s*\"name\": \"([^\"]*).*\s*\"description\": \"([^\"]*)''',content)
  rating = re.findall('''&nbsp;Avg: (...)''',content)
  if len(rating) > 0 and len(info) > 0:
    rating = rating[0]
    if rating[-1] == 'f':
      rating = rating[0]
    name = info[0][0]
    type = info[0][1]
    type = type.split(" ")
    grade = type[0]
    type = type[1:]
    #print("Type: " + str(type))
    if 'Sport' in type:
      sports.append((grade,rating))
    if 'Boulder' in type:
      boulders.append((grade,rating))
    if 'TR' in type:
      topropes.append((grade,rating))
    if 'Trad' in type:
      trads.append((grade,rating))
    print(name + " " + str(type) + " " + grade + " " + rating + "/4 stars")

def try_proxy(url,proxy):
  try:
    response = s.get(url,proxies={"http": proxy, "https": proxy})
    return response
  except:
    return None

# Crawl a link
def crawl(url,proxy_pool):
  if len(boulders) + len(sports) + len(trads) + len(topropes) < num_climbs:
    response = None
    while response == None:
      proxy = next(proxy_pool)
      response = try_proxy(url,proxy)  
 
    page_content = response.content.decode('latin-1')
   
    # Get all links on the page
    links = get_links(page_content)

    # Add the climbs on the page
    get_climbs(page_content)

    # Remove links which we dont want to follow
    for word in ignore:
      links = [link for link in links if not word in link]

    # Only keep route and are links, this will get us all climbs
    links = [link for link in links if 'mountainproject' in link and ('area' in link or 'route' in link)]

    # Add the new links to our set so we don't visit them again
    for link in links:
      if not link in visited:
        visited.add(link)
        if len(visited) % 500 == 0:
          print("\n***\n" + str(len(visited)) + " pages searched... " + str(len(boulders)) + " boulders found... " + str(len(sports)) + " sport routes found... " + str(len(trads)) + " trad routes found... " + str(len(topropes)) + " toprope routes found... \n***\n")
        crawl(link,proxy_pool)

# Prints information about a specific type of climb
def print_type_info(climbs,order,type):
  data = []
  for grade in order:
    sum = 0
    count = 0
    for climb in climbs:
      if climb[0] == grade:
        count += 1
        sum += float(climb[1])
    if count > 0:
      avg = float(float(sum) / float(count))
      data.append((grade,avg))
      print("Average stars for (" + str(count) + ") " + grade + " " + type + ": " + str(round(avg,2)))
  return data

if __name__ == '__main__':
  sys.setrecursionlimit(100000)

  parser = argparse.ArgumentParser()
  parser.add_argument("num_climbs",help="Specifiy the number of climbs to index here")
  args = parser.parse_args()

  num_climbs = int(args.num_climbs)

  # Set up a session
  s = requests.session()

  proxies = get_proxies(s)
  proxy_pool = cycle(proxies)

  crawl('https://www.mountainproject.com',proxy_pool)

  # Sorts lists upon completion
  boulders = [boulder for grade in boulder_order for boulder in boulders if boulder[0] == grade]
  sports = [sport for grade in route_order for sport in sports if sport[0] == grade]
  trads = [trad for grade in route_order for trad in trads if trad[0] == grade]
  topropes = [toprope for grade in route_order for toprope in topropes if toprope[0] == grade]

  # Print meta info
  print('\n\n\nFound in total: ' + str(len(boulders)) + ' boulders, ' + str(len(sports)) + ' sport climbs, ' + str(len(trads)) + ' trad climbs, and ' + str(len(topropes)) + ' toprope climbs.')

  print('\n\n*******************\n* RATING AVERAGES *\n*******************')
  print('\n\n\nBOULDER AVERAGES\n')
  boulder_data = print_type_info(boulders,boulder_order,'boulders')
  print('\n\n\nSPORT CLIMB AVERAGES\n')
  sport_data = print_type_info(sports,route_order,'sport climbs')
  print('\n\n\nTRAD CLIMB AVERAGES\n')
  trad_data = print_type_info(trads,route_order,'trad climbs')
  print('\n\n\nTOPROPE CLIMB AVERAGES\n')
  toprope_data = print_type_info(topropes,route_order,'toprope climbs')
  print('\n\n\n')
